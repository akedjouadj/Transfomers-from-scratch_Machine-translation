{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adjil\\Documents\\DeepL_ComputerV\\Transformers\\Transformers_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from utils import load_checkpoint, save_checkpoint, translate_sentence\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Transformers_from_scratch import Transformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92314</th>\n",
       "      <td>The cat is watching the fish.</td>\n",
       "      <td>[start] Le chat regarde le poisson. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8659</th>\n",
       "      <td>It's well done.</td>\n",
       "      <td>[start] C'est bien fait. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51270</th>\n",
       "      <td>We need some help here.</td>\n",
       "      <td>[start] Nous avons besoin d'aide ici. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129984</th>\n",
       "      <td>I'd like to cash a travelers' check.</td>\n",
       "      <td>[start] J'aimerais encaisser un chèque de voya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33317</th>\n",
       "      <td>Are you freaking out?</td>\n",
       "      <td>[start] Vous avez les foies ? [end]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      source  \\\n",
       "92314          The cat is watching the fish.   \n",
       "8659                         It's well done.   \n",
       "51270                We need some help here.   \n",
       "129984  I'd like to cash a travelers' check.   \n",
       "33317                  Are you freaking out?   \n",
       "\n",
       "                                                   target  \n",
       "92314           [start] Le chat regarde le poisson. [end]  \n",
       "8659                       [start] C'est bien fait. [end]  \n",
       "51270         [start] Nous avons besoin d'aide ici. [end]  \n",
       "129984  [start] J'aimerais encaisser un chèque de voya...  \n",
       "33317                 [start] Vous avez les foies ? [end]  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"english-french-data\"\n",
    "file_path = \"eng-french-data/eng-french.csv\"\n",
    "# read the data\n",
    "df = pd.read_csv(file_path)\n",
    "df['source'] = df['English words/sentences']\n",
    "\n",
    "# let's add an initial “seed” token ([start]) and a stop token ([end]) to each target sentence.\n",
    "df['target'] = df['French words/sentences'].apply(lambda x: '[start] ' + x + ' [end]')\n",
    "df = df.drop(['English words/sentences', 'French words/sentences'], axis=1)\n",
    "\n",
    "# display a few random samples\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175621, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "df = df.sample(frac=0.5).reset_index(drop=True)\n",
    "\n",
    "# split the data into train, validation, and test sets\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.2)\n",
    "test_size = int(len(df) * 0.1)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing, tokenizing and data indexing\n",
    "\n",
    "- First, we need to parse our raw text data and vectorize it. To keep things simple, we will first limit our vocabulary using the max_tokens parameter. We will also limit the length of each sentence using the sequence_length parameter.\n",
    "\n",
    "- Each sentence will be standardized, tokenized by word, and then indexed by token.\n",
    "\n",
    "- This will result in a batch of vectors of tokens, stored in a 2D matrix of shape [(batch_size, sequence_length)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 25000\n",
    "sequence_length = 30\n",
    "\n",
    "# define a custom standardization function that convert to lowercase and strips all punctuations except \"[\" and \"]\" (so we can tell apart \"start\" from \"[start]\").\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    " \n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "# tokenize the data using our custom standardization function\n",
    "source_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1, # add +1 token to our target sentences since they'll be shifted right by 1 during training\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# index all tokens in the source and target sentences\n",
    "train_source_texts = train_df['source'].values\n",
    "train_target_texts = train_df['target'].values\n",
    "source_vectorization.adapt(train_source_texts)\n",
    "target_vectorization.adapt(train_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10134"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(target_vectorization.get_vocabulary()))\n",
    "len(source_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source texts (one random sample): He speaks really well.\n",
      "Target texts (one random sample): [start] Il parle vraiment bien. [end]\n",
      "Source vectors (one random sample): tf.Tensor(\n",
      "[ 10 697  80 110   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0], shape=(30,), dtype=int64)\n",
      "Target vectors (one random sample): tf.Tensor(\n",
      "[  2  13 257  77  76   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(31,), dtype=int64)\n",
      "Source decoded texts (one random sample): he speaks really well                           \n",
      "Target decoded texts (one random sample): [start] il parle vraiment bien [end]                          \n"
     ]
    }
   ],
   "source": [
    "# display a random sample before and after vectorization just to test the vectorization\n",
    "random_sample = random.randint(0, len(train_df))\n",
    "print(\"Source texts (one random sample):\", train_source_texts[random_sample])\n",
    "print(\"Target texts (one random sample):\", train_target_texts[random_sample])\n",
    "print(\"Source vectors (one random sample):\", source_vectorization(train_source_texts[random_sample]))\n",
    "print(\"Target vectors (one random sample):\", target_vectorization(train_target_texts[random_sample]))\n",
    "\n",
    "# display the decoding of the vectorized text (from vector back to text) just to test the vectorization\n",
    "source_decoded_text = ''\n",
    "for i in range(len(source_vectorization(train_source_texts[random_sample]))):\n",
    "    source_decoded_text += source_vectorization.get_vocabulary()[source_vectorization(train_source_texts[random_sample])[i]] + ' '\n",
    "print(\"Source decoded texts (one random sample):\", source_decoded_text)\n",
    "\n",
    "target_decoded_text = ''\n",
    "for i in range(len(target_vectorization(train_target_texts[random_sample]))):\n",
    "    target_decoded_text += target_vectorization.get_vocabulary()[target_vectorization(train_target_texts[random_sample])[i]] + ' '\n",
    "print(\"Target decoded texts (one random sample):\", target_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vectors (shape): torch.Size([61466, 30])\n",
      "Target vectors (shape): torch.Size([61466, 31])\n"
     ]
    }
   ],
   "source": [
    "# display the shape of our vectorized data\n",
    "train_source_vectors = source_vectorization(train_source_texts)\n",
    "train_target_vectors = target_vectorization(train_target_texts)\n",
    "src = torch.from_numpy(train_source_vectors.numpy())\n",
    "target = torch.from_numpy(train_target_vectors.numpy())\n",
    "print(\"Source vectors (shape):\", src.shape)\n",
    "print(\"Target vectors (shape):\", target.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "src_pad_idx = 0\n",
    "target_pad_idx = 0\n",
    "src_vocab_size = len(source_vectorization.get_vocabulary())\n",
    "target_vocab_size = len(target_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (word_embedding): Embedding(10134, 256)\n",
      "    (position_embedding): Embedding(30, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (word_embedding): Embedding(20753, 256)\n",
      "    (position_embedding): Embedding(30, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): SelfAttention(\n",
      "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): SelfAttention(\n",
      "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (attention): SelfAttention(\n",
      "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): SelfAttention(\n",
      "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=20753, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(src_voc_size = src_vocab_size, target_voc_size= target_vocab_size, src_pad_idx = src_pad_idx, \n",
    "                    target_pad_idx = target_pad_idx, embed_size=256, num_layers=3,\n",
    "                     forward_expansion=4, heads = 4, dropout = 0, device = device, \n",
    "                     max_length = sequence_length).to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 6\n",
    "# Création du dataset à partir des données et labels\n",
    "dataset = TensorDataset(src, target, target) \n",
    "# 1st param : encoder inputs\n",
    "# 2nd param : decoder inputs (truncate by 1 to keep it at the same length as decoder_outputs, which is shifted right by 1).\n",
    "# 3rd param : decoder outputs, model target \n",
    "\n",
    "\n",
    "# Création du DataLoader à partir du dataset\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "load_model = True\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source texts (one random sample): I'm going to be your teacher.\n",
      "Target texts (one random sample): [start] Je vais être votre professeur. [end]\n"
     ]
    }
   ],
   "source": [
    "random_sample = random.randint(0, len(train_df))\n",
    "sentence = train_source_texts[random_sample]\n",
    "translation = train_target_texts[random_sample]\n",
    "print(\"Source texts (one random sample):\", sentence)\n",
    "print(\"Target texts (one random sample):\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_(input_sentence, model, source_vectorization, target_vectorization, sequence_length):\n",
    "    tokenized_input_sentence = torch.tensor(source_vectorization([input_sentence]).numpy())\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(sequence_length):\n",
    "        tokenized_target_sentence = torch.tensor(target_vectorization([decoded_sentence])[:, :-1].numpy())\n",
    "        output = model(tokenized_input_sentence, tokenized_target_sentence)\n",
    "        sampled_token_index = output.argmax(2)[0, i].item()\n",
    "        #print(sampled_token_index)\n",
    "        sampled_token = target_vectorization.get_vocabulary()[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "[start] je ne suis pas sûr que cest ton idée [end]\n",
      "[Epoch 0 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [26:46<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 3.321492270301954\n",
      "learning_rate : 0.001\n",
      "[start] je suis très heureux de vous avoir en colère [end]\n",
      "[Epoch 1 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [30:36<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 3.0649833681680163\n",
      "learning_rate : 0.001\n",
      "[start] je suis très heureux de ta part [end]\n",
      "[Epoch 2 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:44<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 2.845444804795947\n",
      "learning_rate : 0.001\n",
      "[start] je suis très heureux de votre avocat [end]\n",
      "[Epoch 3 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [27:45<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 2.646230079132858\n",
      "learning_rate : 0.001\n",
      "[start] je suis très heureux de votre assistance [end]\n",
      "[Epoch 4 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [28:28<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 2.4600819628394976\n",
      "learning_rate : 0.001\n",
      "[start] je vais avoir ton argent [end]\n",
      "[Epoch 5 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [28:21<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 2.287953255451929\n",
      "learning_rate : 0.001\n",
      "[start] je vais vous montrer une autre chance [end]\n",
      "[Epoch 6 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [32:01<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 2.1300605005825965\n",
      "learning_rate : 0.001\n",
      "[start] je vais avoir besoin de ton aide [end]\n",
      "[Epoch 7 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [28:53<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.9905434906420674\n",
      "learning_rate : 0.001\n",
      "[start] je vais le faire par ton voyage [end]\n",
      "[Epoch 8 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:00<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.8585658893376806\n",
      "learning_rate : 0.001\n",
      "[start] je vais vous payer à vos côtés [end]\n",
      "[Epoch 9 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [28:40<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.7447934852802542\n",
      "learning_rate : 0.001\n",
      "[start] je vais arrêter de faire ton travail [end]\n",
      "[Epoch 10 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [28:33<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.638229392941859\n",
      "learning_rate : 0.001\n",
      "[start] je vais être très honnête avec votre famille [end]\n",
      "[Epoch 11 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:11<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.548901650957709\n",
      "learning_rate : 0.001\n",
      "[start] je vais être très honnête [end]\n",
      "[Epoch 12 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:25<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.4735733288011739\n",
      "learning_rate : 0.001\n",
      "[start] je vais être ton professeur [end]\n",
      "[Epoch 13 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:27<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.4050562525639052\n",
      "learning_rate : 0.001\n",
      "[start] je vais te faire du mal [end]\n",
      "[Epoch 14 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:33<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.3462787898224424\n",
      "learning_rate : 0.001\n",
      "[start] je vais être votre institutrice [end]\n",
      "[Epoch 15 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:25<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.2950189976364714\n",
      "learning_rate : 0.001\n",
      "[start] je vais résoudre le problème [end]\n",
      "[Epoch 16 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:24<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.250757617149095\n",
      "learning_rate : 0.001\n",
      "[start] je vais aller chercher ton travail [end]\n",
      "[Epoch 17 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:20<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.2068704744407464\n",
      "learning_rate : 0.001\n",
      "[start] je vais être votre ami [end]\n",
      "[Epoch 18 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:26<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.1696308818369576\n",
      "learning_rate : 0.001\n",
      "[start] je vais aller à la figure [end]\n",
      "[Epoch 19 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [28:57<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.135838264047045\n",
      "learning_rate : 0.001\n",
      "[start] je vais y aller en votre présence [end]\n",
      "[Epoch 20 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:36<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.1021647711946367\n",
      "learning_rate : 0.001\n",
      "[start] je vais accepter votre offre [end]\n",
      "[Epoch 21 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [31:24<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.0727008858406828\n",
      "learning_rate : 0.001\n",
      "[start] je vais aller te chercher [end]\n",
      "[Epoch 22 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [29:20<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss : 1.0450327171867522\n",
      "learning_rate : 0.001\n",
      "[start] je vais aller chercher votre manteau [end]\n",
      "[Epoch 23 / 50]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 233/961 [07:18<22:49,  1.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])\n\u001b[0;32m     36\u001b[0m y \u001b[39m=\u001b[39m y[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, y)\n\u001b[0;32m     40\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     42\u001b[0m \u001b[39m# Back prop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adjil\\Documents\\DeepL_ComputerV\\Transformers\\Transformers_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\adjil\\Documents\\DeepL_ComputerV\\Transformers\\Transformers_venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1164\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1165\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\adjil\\Documents\\DeepL_ComputerV\\Transformers\\Transformers_venv\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "trigger_time = 0\n",
    "patience = 3\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "print(translate_sentence_(sentence, model, source_vectorization, target_vectorization, sequence_length))\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    last_loss = 100\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    # save the model at each epoch, to prevent losing all parameters for early kernel stopping\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for x_enc, x_dec, y in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward prop\n",
    "        output = model(x_enc, x_dec[:, :-1])\n",
    "\n",
    "        # reshape output and y to compute CrossEntropyLoss\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    print(\"mean_loss :\", mean_loss)\n",
    "    print(\"learning_rate :\", learning_rate)\n",
    "    print(translate_sentence(sentence, model, source_vectorization, target_vectorization, sequence_length))\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    # early stopping\n",
    "    if mean_loss > last_loss:\n",
    "        trigger_time+=1\n",
    "        if trigger_time>patience:\n",
    "            break\n",
    "    else:\n",
    "        trigger_time = 0\n",
    "\n",
    "    last_loss = mean_loss # update the last loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_texts = test_df['source'].values\n",
    "test_target_texts = test_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0\n",
      "Source texts : He gave me his word.\n",
      "Model translation : [start] il ma donné une montre ses devoirs [end]\n",
      "Target texts : [start] Il m'a donné sa parole. [end]\n",
      "\n",
      "\n",
      "Sentence 1\n",
      "Source texts : I already told you that a hundred times.\n",
      "Model translation : [start] je vous ai dit que ce nétait pas un endroit [end]\n",
      "Target texts : [start] Je vous ai déjà dit cela cent fois. [end]\n",
      "\n",
      "\n",
      "Sentence 2\n",
      "Source texts : He took a coin out of his pocket.\n",
      "Model translation : [start] il a pris un taxi à boston en vitesse [end]\n",
      "Target texts : [start] Il sortit une pièce de sa poche. [end]\n",
      "\n",
      "\n",
      "Sentence 3\n",
      "Source texts : The water from this fountain is safe to drink.\n",
      "Model translation : [start] la nourriture depuis que la vie est vraiment mary [end]\n",
      "Target texts : [start] L'eau de cette fontaine est potable. [end]\n",
      "\n",
      "\n",
      "Sentence 4\n",
      "Source texts : I think I'll be busy this week.\n",
      "Model translation : [start] je pense que je pourrais jouer avec elle [end]\n",
      "Target texts : [start] Je pense que je vais être occupée cette semaine. [end]\n",
      "\n",
      "\n",
      "Sentence 5\n",
      "Source texts : The strike affected the nation's economy.\n",
      "Model translation : [start] la guerre sest déclenchée en 1939 [end]\n",
      "Target texts : [start] La grève a affecté l'économie nationale. [end]\n",
      "\n",
      "\n",
      "Sentence 6\n",
      "Source texts : As you can see, I haven't done any cleaning in the house for some time.\n",
      "Model translation : [start] au veuxtu que je suis venu au courant que je vais pouvoir aller au bureau [end]\n",
      "Target texts : [start] Comme vous pouvez voir, je n'ai pas procédé au nettoyage de la maison depuis un certain temps. [end]\n",
      "\n",
      "\n",
      "Sentence 7\n",
      "Source texts : Tom waited for Mary for three hours.\n",
      "Model translation : [start] tom a donné à mary de largent à marie [end]\n",
      "Target texts : [start] Tom a attendu Mary pendant trois heures. [end]\n",
      "\n",
      "\n",
      "Sentence 8\n",
      "Source texts : We ran out of money.\n",
      "Model translation : [start] nous avons marché autour de létang [end]\n",
      "Target texts : [start] Nous nous trouvâmes à court d'argent. [end]\n",
      "\n",
      "\n",
      "Sentence 9\n",
      "Source texts : Don't say I didn't warn you.\n",
      "Model translation : [start] ne dites pas que je ne vous aurai pas prévenus [end]\n",
      "Target texts : [start] Ne dites pas que je ne vous aurai pas prévenues ! [end]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sample = np.random.randint(0, len(test_df), 10)\n",
    "for k, i in enumerate(random_sample):\n",
    "    sentence = test_source_texts[i]\n",
    "    model_translation = translate_sentence(sentence, model, source_vectorization, target_vectorization, sequence_length)\n",
    "    real_translation = test_target_texts[i]\n",
    "    print(f\"Sentence {k}\")\n",
    "    print(\"Source texts :\", sentence)\n",
    "    print(\"Model translation :\", model_translation)\n",
    "    print(\"Target texts :\", real_translation)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
